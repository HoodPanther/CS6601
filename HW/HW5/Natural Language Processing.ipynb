{
 "metadata": {
  "name": "Natural Language Processing"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Natural Language Processing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Ruffin White  \n",
      "CS6601  \n",
      "HW5"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "NGram Models and Grammars"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this homework we will look into two approaches two natural language processing. One approach is looking at a document as a loose collection of words or a loose collection of strings of words called an ngram. Furthermore, we will explore parsing using context free grammars and their relation to hidden Markov models. The reading associated with this homework is AIMA chapter 22 and 23."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "1) Markov Model vs Probabilistic Context-free Grammar: 20 Points"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Probabilistic context-free grammars (PCFG) are a super set of Markov models (probabilistic\n",
      "regular grammars). Write down the PCFG for the following Markov model."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "2) Hidden Markov Model vs Probabilistic Context-free Grammar: 20 Points"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Recall the lecture and reading on hidden Markov models. What is the equivalent of the Viterbi algorithm for probabilistic context-free grammars? What is the equivalent of the Baum Welch algorithm for grammars? Can you represent a hidden Markov model as a PCFG and if yes how? Write one paragraph for each of the questions and explain your answer."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "3) Expressiveness of Hidden Markov Models and NGrams: 10 Points"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Describe the di\u000b",
      "erence as well as similarities between hidden Markov models and ngrams. Particularly, how does the HMM topology relate to relate to bigrams. Are there transitions, too? Argue using Markov order."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "4) NGram estimation: 50 Points"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "AIMA 22.1) This exercise explores the quality of the n-gram model of language. Find or create a monolingual corpus of 100,000 words or more. Segment it into words, and compute the frequency of each word. How many distinct words are there? Also count frequencies of bigrams (two consecutive words) and trigrams (three consecutive words). Now use those frequencies to generate language: from the unigram, bigram, and trigram models, in turn, generate a 100-word text by making random choices according to the frequency counts. Compare the three generated texts with actual language. Finally, calculate the perplexity of each model."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}